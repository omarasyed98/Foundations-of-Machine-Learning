{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for KNN: 0.9635036496350365\n",
      "548/548 [==============================] - 0s 9us/step\n",
      "\n",
      "acc: 98.36%\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import sklearn\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "data = pandas.read_csv('Final.txt', sep=',', names = ['1','2','Color'])\n",
    "#data\n",
    "#reading data from csv file to pandas dataframe\n",
    "X = data.drop('Color', axis=1)\n",
    "Y = data['Color']\n",
    "#seperating data into inputs and output values\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "Y = le.fit_transform(Y)\n",
    "#using sklearn preprocessing to encode non-numerical values to numerical values\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "# #splitting data into training set and test set\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\n",
    "# #using minikowski of order 2 for knn classifier from sklearn\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "#fitting data to knn classifier\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "# #making predictions on test data based on training data\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy for KNN:\",metrics.accuracy_score(y_test, y_pred))\n",
    "#reporting accuracy for simple KNN model\n",
    "\n",
    "model = Sequential()\n",
    "#using model from keras to build neural network\n",
    "model.add(Dense(50, input_dim=2, activation='relu'))\n",
    "#starting first layer with 2 input dimensions and creating 50 neurons from them using relu activation\n",
    "model.add(Dense(50, activation='sigmoid'))\n",
    "#adding 2nd layer with 50 neurons as well with sigmoid activation. I tried many different activations and sigmoid by far worked the best and when the number of neurons were the same in\n",
    "#layers 1 and 2 it also seemed to work better\n",
    "#model.add(Dense(40, activation='relu'))\n",
    "#adding a third layer seemed to have no increase in accuracy\n",
    "model.add(Dense(1, activation='relu'))\n",
    "#final output layer which returns our one output desired\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "#defining learning rate for optimizer\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer='adam', metrics=['accuracy'])\n",
    "#compiling model defined above. I tried many differnt loss functions and optimization algorithms but these seemed to work the best by far\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=5, verbose =0)\n",
    "#Fitting the model to a training set and test set, saving 20% of the data to test on. Setting the Epochs to 100 because after 100 there seems to be little increase in accuracy\n",
    "#Verbose was changed for clarity of display, I used verbose 1 when actually changing parameters\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#Evaulating model's performance\n",
    "\n",
    "# y_pred=model.predict(X_test)\n",
    "\n",
    "# from sklearn import metrics\n",
    "\n",
    "# print(\"Accuracy for Test Data:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for KNN: 0.9708029197080292\n",
      "548/548 [==============================] - 0s 11us/step\n",
      "\n",
      "acc: 93.07%\n"
     ]
    }
   ],
   "source": [
    "#MANY OF THE SAME METHODS WERE EMPLOYED FOR ALL ATTRIBUTES, NOTES FROM FIRST APPLY TO ALL\n",
    "import keras\n",
    "import sklearn\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "data = pandas.read_csv('Final.txt', sep=',', names = ['X1','X2','Color'])\n",
    "#data\n",
    "\n",
    "\n",
    "\n",
    "X1 = data['X1']\n",
    "X2 = data['X2']\n",
    "X3 = X1*X1\n",
    "X4 = X2*X2\n",
    "X5 = X1*X2\n",
    "Y = data['Color']\n",
    "#creating new attributes as required by the project guidelines\n",
    "\n",
    "Attribute1 = pandas.DataFrame({'X3': X3, 'X4': X4})\n",
    "#making new attribute to use as training data to produce an output\n",
    "#bob = pandas.DataFrame(X3) + pandas.DataFrame(X4)\n",
    "\n",
    "\n",
    "#Attribute1\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Attribute1, Y, test_size=0.2)\n",
    "#splitting data into training set and test set\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\n",
    "# #using minikowski of order 2 for knn classifier from sklearn\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "# #fitting data to knn classifier\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "# #making predictions on test data based on training data\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy for KNN:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=2, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "#model.add(Dense(20, activation='relu'))\n",
    "#model.add(Dense(80, activation='linear'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "model.compile(loss='poisson', optimizer='adam', metrics=['accuracy'])\n",
    "#Again I tried many different variations of activiations, loss functions, neurons, layers, optimizers, and I settled after I could not improve the accuracy by much.\n",
    "\n",
    "model.fit(X_train, y_train, validation_split = 0.2, epochs=100, batch_size=50, verbose = 0)\n",
    "\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n",
    "# y_pred=model.predict(X_test)\n",
    "\n",
    "# from sklearn import metrics\n",
    "\n",
    "# print(\"Accuracy for Test Data:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for KNN: 0.9781021897810219\n",
      "548/548 [==============================] - 0s 9us/step\n",
      "\n",
      "acc: 93.25%\n"
     ]
    }
   ],
   "source": [
    "#MANY OF THE SAME METHODS WERE EMPLOYED FOR ALL ATTRIBUTES, NOTES FROM FIRST APPLY TO ALL\n",
    "import keras\n",
    "import sklearn\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "data = pandas.read_csv('Final.txt', sep=',', names = ['X1','X2','Color'])\n",
    "#data\n",
    "\n",
    "\n",
    "\n",
    "X1 = data['X1']\n",
    "X2 = data['X2']\n",
    "X3 = X1*X1\n",
    "X4 = X2*X2\n",
    "X5 = X1*X2\n",
    "Y = data['Color']\n",
    "\n",
    "\n",
    "Attribute2 = pandas.DataFrame({'X3': X3, 'X5': X5})\n",
    "#bob = pandas.DataFrame(X3) + pandas.DataFrame(X4)\n",
    "#making new attribute to use as training data to produce an output\n",
    "#Attribute2\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Attribute2, Y, test_size=0.2)\n",
    "#splitting data into training set and test set\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\n",
    "# #using minikowski of order 2 for knn classifier from sklearn\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "# #fitting data to knn classifier\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "# #making predictions on test data based on training data\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy for KNN:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=2, activation='relu'))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "#model.add(Dense(60, activation='sigmoid'))\n",
    "#model.add(Dense(80, activation='linear'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "from keras import optimizers\n",
    "adam = optimizers.adam(lr=1)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])\n",
    "#Again I tried many different variations of activiations, loss functions, neurons, layers, optimizers, and I settled after I could not improve the accuracy by much.\n",
    "\n",
    "model.fit(X_train, y_train, validation_split = 0.2, epochs=300, batch_size=50, verbose = 0)\n",
    "\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "# y_pred=model.predict(X_test)\n",
    "\n",
    "# from sklearn import metrics\n",
    "\n",
    "# print(\"Accuracy for Test Data:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for KNN: 0.9854014598540146\n",
      "548/548 [==============================] - 0s 15us/step\n",
      "\n",
      "acc: 93.07%\n"
     ]
    }
   ],
   "source": [
    "#MANY OF THE SAME METHODS WERE EMPLOYED FOR ALL ATTRIBUTES, NOTES FROM FIRST APPLY TO ALL\n",
    "import keras\n",
    "import sklearn\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "data = pandas.read_csv('Final.txt', sep=',', names = ['X1','X2','Color'])\n",
    "#data\n",
    "\n",
    "\n",
    "\n",
    "X1 = data['X1']\n",
    "X2 = data['X2']\n",
    "X3 = X1*X1\n",
    "X4 = X2*X2\n",
    "X5 = X1*X2\n",
    "Y = data['Color']\n",
    "\n",
    "\n",
    "Attribute3 = pandas.DataFrame({'X3': X3, 'X4': X4,'X5': X5})\n",
    "#bob = pandas.DataFrame(X3) + pandas.DataFrame(X4)\n",
    "#making new attribute to use as training data to produce an output\n",
    "#Attribute3\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Attribute3, Y, test_size=0.2)\n",
    "#splitting data into training set and test set\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\n",
    "# #using minikowski of order 2 for knn classifier from sklearn\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "# #fitting data to knn classifier\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "# #making predictions on test data based on training data\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy for KNN:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=3, activation='relu'))\n",
    "model.add(Dense(50, activation='sigmoid'))\n",
    "model.add(Dense(100, activation='linear'))\n",
    "#model.add(Dense(80, activation='linear'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "from keras import optimizers\n",
    "adam = optimizers.adam(lr=0.1)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])\n",
    "#Again I tried many different variations of activiations, loss functions, neurons, layers, optimizers, and I settled after I could not improve the accuracy by much.\n",
    "\n",
    "model.fit(X_train, y_train, validation_split = 0.2, epochs=50, batch_size=50, verbose = 0)\n",
    "\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "# y_pred=model.predict(X_test)\n",
    "\n",
    "# from sklearn import metrics\n",
    "\n",
    "# print(\"Accuracy for Test Data:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for KNN: 0.9927007299270073\n",
      "548/548 [==============================] - 0s 20us/step\n",
      "\n",
      "acc: 92.52%\n"
     ]
    }
   ],
   "source": [
    "#MANY OF THE SAME METHODS WERE EMPLOYED FOR ALL ATTRIBUTES, NOTES FROM FIRST APPLY TO ALL\n",
    "import keras\n",
    "import sklearn\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "data = pandas.read_csv('Final.txt', sep=',', names = ['X1','X2','Color'])\n",
    "#data\n",
    "\n",
    "\n",
    "\n",
    "X1 = data['X1']\n",
    "X2 = data['X2']\n",
    "X3 = X1*X1\n",
    "X4 = X2*X2\n",
    "X5 = X1*X2\n",
    "Y = data['Color']\n",
    "\n",
    "\n",
    "Attribute4 = pandas.DataFrame({'X1': X1,'X2': X2,'X3': X3, 'X4': X4,'X5': X5})\n",
    "#bob = pandas.DataFrame(X3) + pandas.DataFrame(X4)\n",
    "#making new attribute to use as training data to produce an output\n",
    "#Attribute4\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "Y = le.fit_transform(Y)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Attribute4, Y, test_size=0.2)\n",
    "#splitting data into training set and test set\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\n",
    "# #using minikowski of order 2 for knn classifier from sklearn\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "# #fitting data to knn classifier\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "# #making predictions on test data based on training data\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy for KNN:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=5, activation='relu'))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "#model.add(Dense(40, activation='linear'))\n",
    "#model.add(Dense(20, activation='elu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "from keras import optimizers\n",
    "adam = optimizers.adam(lr=0.1)\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer='adam', metrics=['accuracy'])\n",
    "#Again I tried many different variations of activiations, loss functions, neurons, layers, optimizers, and I settled after I could not improve the accuracy by much.\n",
    "\n",
    "model.fit(X_train, y_train, validation_split = 0.2, epochs=100, batch_size=50, verbose = 0)\n",
    "\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "# y_pred=model.predict(X_test)\n",
    "\n",
    "# from sklearn import metrics\n",
    "\n",
    "# print(\"Accuracy for Test Data:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONCLUSION\n",
    "#From what I can tell the dataset was relatively incomplex as the KNN algorithm in some cases was able to compute at almost 100% accuracy. However, for the neural network it got\n",
    "#progressively harder to keep up the KNN algorithm as we added more complexity to the variables by creating new attributes. Also I noticided a lot of trend in setting the parameters\n",
    "#for the neural models. For example, if at least one layer in every model I tried did not have a sigmoid activation the accuracy was very low. However, using multiple sigmoid functions\n",
    "#also would produce very low accuracy. It was a great balancing act to fine tune the parameters for each differing input sets. Another interesting thing I noticed was that at least for \n",
    "#this simple of a dataset when I more than 3 layers in total to the model the accuracy would not increase by much and many times it would even decrease. All in all I found neural nets\n",
    "#to be very complex in the way they can be tuned specifically to your data vs something that is much more plug and play like a KNN algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
